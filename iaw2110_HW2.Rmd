---
title: "HW2"
author: "Ivan Wolansky"
date: "2/24/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Problem 1 (Bayes-Optimal Classifier)**

From the hints, we have $$R(f) = \int_{R^{d}} R(f|x)p(x)dx \quad \textrm{ where } \quad R(f|x) = \sum_{k \in [K]} L^{0-1}(y,f(x))P(y|x)$$ From the hints we also know that if we minimize $R(f|x)$ such that $$f_{0} = arg min_{f \in H} R(f|x) \quad \forall x \in R^{d}$$ then $f_{0}$ minimizes $R(f)$.\newline

Next, we show: $$R(f|x) = \sum_{k \in [K]} L^{0-1}(y,f(x))P(y|x)$$\newline 
$$ = P(y=f(x)|x) \underbrace{L^{0-1}(f(x), f(x))}_\text{= 0} + \sum_{k\neq f(x)} P(k|x) \underbrace{L^{0-1}(f(x), k)}_\text{= 1}$$\newline
$$ = \sum_{k\neq f(x)} P(k|x)$$\newline
We know $\sum_{k\neq f(x)} P(k|x)$ = 1 over all k so $$R(f|x) = 1 - P(f(x)|x)$$ Our definition in the problem said that $f_{0}(x) = arg max_{y \in [K]} P(y|x)$ which means that given the above result, $f_{0}$ must also minimize $1 - P(f(x)|x)$ at every x. This means: $$f_{0} = min_{f \in H} R(f|x) \quad \forall x \in R^{d}$$ and given the hint, we therefore can conclude: $$f_{0} = min_{f \in H} R(f)$$

**Problem 2 (Non-linear Decision Boudary)**
![Sketch of Function (Parts a and b)](C:/Users/iawol/Desktop/College/Senior/2nd Semester/ML/HW2/function_sketch.jpg)

c.
(0,0) is classified to the blue class, (-1, 1) is classified to the red class, (2, 2) is classified to the blue class, and (3, 8) is classified to the blue class.

d. We can clearly see that the decision boundary is not linear in terms of $X_{1} \textrm{ and } X_{2}$ because the boundary in $$(1+X_{1})^{2}+(2-X_{2})^{2}=4$$ has quadratic terms in it. However, if we expand this boundary we get: $$1 + 2X_{1} + X_{1}^{2} + 4 - 4X_{2} + X_{2}^{2} = 4 \rightarrow 1 + 2X_{1} + X_{1}^{2} - 4X_{2} + X_{2}^{2} = 0$$. $1 + 2X_{1} + X_{1}^{2} - 4X_{2} + X_{2}^{2} = 0$ is linear in terms of $X_{1}, X_{2}, X_{1}^{2}, \textrm{ and } X_{2}^{2}$

**Problem 3 (LDA and Logistic Regression)**

```{r}
library(MASS)
library(glmnet)
```

```{r}
# reading in data and making a y column depending on the corresponding data
set.seed(4400)
zip_3 <- read.table("train_3.txt", header=FALSE, sep=",")
zip_3 <-as.matrix(cbind(zip_3, rep(3, nrow(zip_3))))
colnames(zip_3)[257] <- 'y'
zip_5 <-read.table("train_5.txt", header=FALSE, sep=",")
zip_5 <-as.matrix(cbind(zip_5, rep(5, nrow(zip_5))))
colnames(zip_5)[257] <- 'y'
zip_8 <-read.table("train_8.txt", header=FALSE, sep=",")
zip_8 <-as.matrix(cbind(zip_8, rep(8, nrow(zip_8))))
colnames(zip_8)[257] <- 'y'
```

```{r}
# combining data into one matrix
data <- rbind(zip_3, zip_5, zip_8)
```

**1.**

```{r}
# sampling the data to make the train and test sets
sample <- sample.int(nrow(data), size = floor(0.8 * nrow(data)), replace = F)
train <- data[sample, ]
test <- data[-sample, ]
```

```{r}
# splitting train and test into the x values and the variable labels
X_train <- train[, 1:256]
y_train <- train[, 257]
X_test <- test[, 1:256]
y_test <- test[, 257]
```

```{r}
lda_fit256 <- lda(X_train, y_train)
lda_pred_train_256 <- predict(lda_fit256, X_train)
lda_pred_test_256 <- predict(lda_fit256, X_test)
confusion_train_256 <- table(lda_pred_train_256$class, y_train)
confusion_test_256 <- table(lda_pred_test_256$class, y_test)

errors <- data.frame(Model = NA, training_error = NA, testing_error = NA)[FALSE,]
errors[1,] <- c("Full LDA", mean(lda_pred_train_256$class != y_train), mean(lda_pred_test_256$class != y_test))
```

**2.**

```{r}
# performing pca and combining the adjusted data with the label
pca <- prcomp(data[, 1:256], scale. = TRUE)
pca_and_y <- cbind(pca$x[, 1:49], data[, 257])
colnames(pca_and_y)[50] <- "y"
```

```{r}
# sampling the data to make the train and test sets
sample_pca <- sample.int(nrow(pca_and_y), size = floor(0.8 * nrow(pca_and_y)), replace = F)
train_pca <- pca_and_y[sample_pca, ]
test_pca <- pca_and_y[-sample_pca, ]
```

```{r}
# splitting train and test into the x values and the variable labels
X_train_pca <- train_pca[, 1:49]
y_train_pca <- train_pca[, 50]
X_test_pca <- test_pca[, 1:49]
y_test_pca <- test_pca[, 50]
```

```{r}
lda_fit_pca <- lda(X_train_pca, y_train_pca)
lda_pred_train_pca <- predict(lda_fit_pca, X_train_pca)
lda_pred_test_pca <- predict(lda_fit_pca, X_test_pca)
confusion_train_pca <- table(lda_pred_train_pca$class, y_train_pca)
confusion_test_pca <- table(lda_pred_test_pca$class, y_test_pca)


errors[2,] <- c("49 Principal Components LDA", mean(lda_pred_train_pca$class != y_train_pca), mean(lda_pred_test_pca$class != y_test_pca))
```

**3.**

```{r}
h1<-kronecker(diag(8),cbind(c(0.5,0.5)))
h2<-kronecker(h1,h1)
X_train_filter <- I(X_train%*%h2)
X_test_filter <- I(X_test%*%h2)
lda_fit_filter <- lda(X_train_filter, y_train)
lda_pred_train_filter <- predict(lda_fit_filter, X_train_filter)
lda_pred_test_filter <- predict(lda_fit_filter, X_test_filter)
confusion_train_filter <- table(lda_pred_train_filter$class, y_train)
confusion_test_filter <- table(lda_pred_test_filter$class, y_test)

errors[3,] <- c("Filtered LDA", mean(lda_pred_train_filter$class != y_train), mean(lda_pred_test_filter$class != y_test))
```

**4.**

```{r}
mlr_cv <- cv.glmnet(x = X_train_filter, 
                    y = y_train,
                    nfolds = 5,
                    type.measure = 'class',
                    alpha = 1,
                    family = 'multinomial')

errors[4,] <- c("Logistic Regression", mean(predict(mlr_cv, X_train_filter, s = mlr_cv$lambda.min, type = "class") != y_train), mean(predict(mlr_cv, X_test_filter, s = mlr_cv$lambda.min, type = "class") != y_test))
errors
```
For this seed and all 256 dimensions, the misclassification rate for training is 0.0163817663817664 and for testing it is 0.0340909090909091.

For this seed and the lead 49 principal components, the misclassification rate for training is 0.0448717948717949 and for testing it is 0.0539772727272727.

For this seed and the filtering, the misclassification rate for training is 0.0349002849002849 and for testing it is 0.0426136363636364.

For this seed and the logistic regresseion, the misclassification rate for training is 0.0199430199430199 and for testing it is 0.0340909090909091.